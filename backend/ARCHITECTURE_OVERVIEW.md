# Library of Congress Prototype - Architecture Overview

## System Overview

The Library of Congress prototype is a **serverless RAG (Retrieval-Augmented Generation) pipeline** that collects historical documents from multiple sources and provides AI-powered chat capabilities. The system is built entirely on AWS using modern cloud-native technologies.

## High-Level Architecture

```
┌─────────────────────────────────────────────────────────┐
│                    Frontend (React)                     │
│  • Persona-based chat interface                         │
│  • Real-time responses                                  │
│  • Deployed on AWS Amplify                             │
└─────────────────────┬───────────────────────────────────┘
                      │ HTTPS/REST API
                      ▼
┌─────────────────────────────────────────────────────────┐
│                 API Gateway + Lambda                    │
│  • Chat Handler (persona-based responses)              │
│  • Fargate Trigger (data collection)                   │
│  • KB Sync Trigger (knowledge base management)         │
└─────────────────────┬───────────────────────────────────┘
                      │
                      ▼
┌─────────────────────────────────────────────────────────┐
│              Knowledge Base (Bedrock)                   │
│  • Neptune Analytics (vector storage)                  │
│  • Titan Embed v2 (embeddings)                        │
│  • Claude 3 Haiku (chat completions)                  │
└─────────────────────┬───────────────────────────────────┘
                      │
                      ▼
┌─────────────────────────────────────────────────────────┐
│                Data Collection Layer                    │
│  • ECS Fargate (containerized collectors)              │
│  • Amazon Textract (PDF text extraction)              │
│  • S3 (document storage)                              │
└─────────────────────────────────────────────────────────┘
```

## Core Technologies & Resource Usage

### Infrastructure & Deployment
- **AWS CDK (TypeScript)** - Infrastructure as Code
  - *Usage*: Defines entire stack in `chronicling-america-stack.ts`
  - *Integration*: Provisions all AWS resources with proper IAM roles and networking
- **AWS CodeBuild** - CI/CD pipeline
  - *Usage*: Automated deployment via `buildspec.yml`
  - *Integration*: Builds Docker images, deploys CDK, manages frontend deployment
- **CloudFormation** - Resource provisioning
  - *Usage*: Underlying infrastructure management (generated by CDK)
  - *Integration*: Handles resource dependencies and rollback capabilities
- **AWS Amplify** - Frontend hosting
  - *Usage*: Hosts React frontend with automatic builds
  - *Integration*: Connected to S3 builds bucket for deployment artifacts

### Data Processing
- **Amazon ECS Fargate** - Containerized data collection
  - *Usage*: Runs Python collectors in isolated containers (2 vCPU, 4GB RAM)
  - *Integration*: Triggered by Lambda, writes to S3, logs to CloudWatch
- **Amazon Textract** - PDF/image text extraction
  - *Usage*: Processes 2 API types based on file size (<5MB sync, >5MB async)
  - *Integration*: Called from Fargate tasks, results stored in S3 with metadata
- **Python 3.11** - Primary programming language
  - *Usage*: All Lambda functions and Fargate tasks
  - *Integration*: Boto3 SDK for AWS service integration
- **Docker** - Container runtime
  - *Usage*: Packages Fargate applications with dependencies
  - *Integration*: Built and pushed to ECR during deployment

### AI & Knowledge Management
- **Amazon Bedrock Knowledge Base** - Vector database
  - *Usage*: Stores 1024-dimensional embeddings with metadata filtering
  - *Integration*: Connected to Neptune Analytics, triggered by S3 events
- **Neptune Analytics** - Graph database with vector search
  - *Usage*: 16 m-NCUs provisioned memory, private connectivity
  - *Integration*: Backend storage for Knowledge Base with vector search capabilities
- **Amazon Titan Embed v2** - Text embeddings (1024 dimensions)
  - *Usage*: Converts text chunks to vectors for similarity search
  - *Integration*: Automatically invoked during Knowledge Base ingestion
- **Anthropic Claude 3 Haiku** - Language model
  - *Usage*: Generates persona-based responses and entity extraction
  - *Integration*: Called via Bedrock API with custom prompts

### Serverless Computing
- **AWS Lambda** - Event-driven functions
  - *Usage*: 4 functions (chat, fargate-trigger, kb-sync, transformation)
  - *Integration*: API Gateway triggers, S3 events, manual invocation
- **API Gateway** - REST API endpoints
  - *Usage*: Exposes `/chat` and `/collect` endpoints
  - *Integration*: Routes requests to Lambda with CORS enabled
- **EventBridge** - Event routing
  - *Usage*: Coordinates between data collection and KB sync
  - *Integration*: Triggers Lambda functions based on S3 events
- **CloudWatch** - Logging and monitoring
  - *Usage*: Centralized logging for all components
  - *Integration*: Automatic log aggregation with structured logging

## Resource Integration Patterns

### 1. CDK Infrastructure Integration

The entire system is defined as Infrastructure as Code using AWS CDK:

```typescript
// chronicling-america-stack.ts - Resource definitions and integrations

// VPC and Networking
const vpc = new ec2.Vpc(this, "VPC", {
  maxAzs: 2,
  natGateways: 1  // Cost optimization
});

// ECS Cluster for Fargate tasks
const cluster = new ecs.Cluster(this, "Cluster", {
  vpc: vpc,
  containerInsights: true  // CloudWatch integration
});

// S3 Buckets with proper lifecycle policies
const dataBucket = new s3.Bucket(this, "DataBucket", {
  versioning: false,
  lifecycleRules: [{
    id: "DeleteIncompleteUploads",
    abortIncompleteMultipartUploadAfter: cdk.Duration.days(1)
  }]
});

// Lambda functions with proper IAM roles
const chatHandler = new PythonFunction(this, "ChatHandlerFunction", {
  entry: path.join(__dirname, "../lambda/chat-handler"),
  runtime: lambda.Runtime.PYTHON_3_11,
  timeout: cdk.Duration.minutes(5),
  memorySize: 1024,
  environment: {
    KNOWLEDGE_BASE_ID: "", // Updated post-deployment
    MODEL_ID: bedrockModelId
  }
});

// API Gateway with Lambda integration
const api = new apigateway.RestApi(this, "Api", {
  restApiName: `${projectName}-api`,
  defaultCorsPreflightOptions: {
    allowOrigins: apigateway.Cors.ALL_ORIGINS,
    allowMethods: apigateway.Cors.ALL_METHODS
  }
});

// Connect Lambda to API Gateway
const chatResource = api.root.addResource("chat");
chatResource.addMethod("POST", new apigateway.LambdaIntegration(chatHandler));
```

### 2. Lambda Function Integrations

#### Chat Handler Integration Pattern
```python
# lambda/chat-handler/lambda_function.py

import boto3
import json
import os

# AWS service clients initialization
bedrock_agent_runtime = boto3.client('bedrock-agent-runtime')
bedrock_runtime = boto3.client('bedrock-runtime')

def lambda_handler(event, context):
    # 1. Parse API Gateway event
    body = json.loads(event.get('body', '{}'))
    question = body.get('question', '')
    persona = body.get('persona', 'general')
    
    # 2. Extract bill information using regex patterns
    bill_info = extract_bill_info(question)
    
    # 3. Build Knowledge Base metadata filter
    metadata_filter = build_metadata_filter(bill_info)
    
    # 4. Query Bedrock Knowledge Base
    kb_response = bedrock_agent_runtime.retrieve_and_generate(
        input={'text': question},
        retrieveAndGenerateConfiguration={
            'type': 'KNOWLEDGE_BASE',
            'knowledgeBaseConfiguration': {
                'knowledgeBaseId': os.environ['KNOWLEDGE_BASE_ID'],
                'modelArn': f"arn:aws:bedrock:{region}::foundation-model/{model_id}",
                'retrievalConfiguration': {
                    'vectorSearchConfiguration': {
                        'filter': metadata_filter  # Precise filtering
                    }
                }
            }
        }
    )
    
    # 5. Return formatted response
    return {
        'statusCode': 200,
        'headers': {'Content-Type': 'application/json'},
        'body': json.dumps(response_data)
    }
```

#### Fargate Trigger Integration
```python
# lambda/fargate-trigger/lambda_function.py

def lambda_handler(event, context):
    # 1. Parse trigger parameters
    body = json.loads(event.get('body', '{}'))
    start_congress = body.get('start_congress', 1)
    end_congress = body.get('end_congress', 16)
    
    # 2. Launch ECS Fargate task
    ecs_client = boto3.client('ecs')
    
    response = ecs_client.run_task(
        cluster=os.environ['CLUSTER_NAME'],
        taskDefinition=os.environ['TASK_DEFINITION_ARN'],
        launchType='FARGATE',
        networkConfiguration={
            'awsvpcConfiguration': {
                'subnets': [os.environ['SUBNET_ID']],
                'securityGroups': [os.environ['SECURITY_GROUP_ID']],
                'assignPublicIp': 'ENABLED'
            }
        },
        overrides={
            'containerOverrides': [{
                'name': 'collector',
                'environment': [
                    {'name': 'START_CONGRESS', 'value': str(start_congress)},
                    {'name': 'END_CONGRESS', 'value': str(end_congress)},
                    {'name': 'BUCKET_NAME', 'value': os.environ['BUCKET_NAME']}
                ]
            }]
        }
    )
    
    return {'statusCode': 200, 'body': json.dumps({'taskArn': response['tasks'][0]['taskArn']})}
```

### 3. Fargate Data Collection Integration

#### Multi-Source Data Collection
```python
# fargate/collect_bills.py

class DocumentCollector:
    def __init__(self):
        self.s3_client = boto3.client('s3')
        self.textract_client = boto3.client('textract')
        self.bedrock_agent_client = boto3.client('bedrock-agent')
        
    def collect_congress_bills(self):
        """Collect from Congress API with Textract integration"""
        for congress in range(self.start_congress, self.end_congress + 1):
            for bill_type in self.bill_types:
                # 1. Fetch bill metadata from Congress API
                bills = self.fetch_congress_bills(congress, bill_type)
                
                for bill in bills:
                    # 2. Download PDF if available
                    pdf_url = bill.get('textVersions', [{}])[0].get('formats', [{}])[0].get('url')
                    if pdf_url:
                        # 3. Process with Textract
                        text_content = self.extract_text_with_textract(pdf_url)
                        
                        # 4. Build metadata
                        metadata = {
                            'congress': str(congress),
                            'bill_type': bill_type,
                            'bill_number': str(bill['number']),
                            'title': bill.get('title', ''),
                            'date': bill.get('introducedDate', ''),
                            'source': 'congress_api'
                        }
                        
                        # 5. Upload to S3 with metadata
                        self.upload_to_s3(text_content, metadata, congress, bill_type, bill['number'])
    
    def extract_text_with_textract(self, pdf_url):
        """Smart Textract integration based on file size"""
        # Download and check file size
        response = requests.get(pdf_url)
        file_size = len(response.content)
        
        if file_size <= 5 * 1024 * 1024:  # 5MB limit
            # Use synchronous API for small files
            return self.textract_sync(response.content)
        else:
            # Use asynchronous API for large files
            return self.textract_async(response.content)
    
    def textract_sync(self, pdf_content):
        """Synchronous Textract processing"""
        response = self.textract_client.detect_document_text(
            Document={'Bytes': pdf_content}
        )
        
        # Extract text from response
        text_blocks = []
        for block in response['Blocks']:
            if block['BlockType'] == 'LINE':
                text_blocks.append(block['Text'])
        
        return '\n'.join(text_blocks)
    
    def upload_to_s3(self, content, metadata, congress, bill_type, bill_number):
        """Upload with proper S3 organization"""
        key = f"extracted/congress_{congress}/{bill_type}/{bill_type}_{bill_number}.txt"
        
        self.s3_client.put_object(
            Bucket=self.bucket_name,
            Key=key,
            Body=content,
            Metadata=metadata,  # S3 metadata for Knowledge Base
            ContentType='text/plain'
        )
        
        # Trigger Knowledge Base sync if this is the last document
        if self.should_trigger_sync():
            self.trigger_kb_sync()
```

### 4. Knowledge Base Integration Architecture

#### Neptune Analytics + Bedrock Integration
```typescript
// CDK configuration for Knowledge Base
const knowledgeBaseRole = new iam.Role(this, "KnowledgeBaseRole", {
  assumedBy: new iam.ServicePrincipal("bedrock.amazonaws.com"),
  inlinePolicies: {
    NeptuneAnalyticsAccess: new iam.PolicyDocument({
      statements: [
        new iam.PolicyStatement({
          effect: iam.Effect.ALLOW,
          actions: [
            "neptune-graph:ReadDataViaQuery",
            "neptune-graph:WriteDataViaQuery",
            "neptune-graph:DeleteDataViaQuery"
          ],
          resources: [neptuneGraphArn]
        })
      ]
    }),
    S3Access: new iam.PolicyDocument({
      statements: [
        new iam.PolicyStatement({
          effect: iam.Effect.ALLOW,
          actions: ["s3:GetObject", "s3:ListBucket"],
          resources: [dataBucket.bucketArn, `${dataBucket.bucketArn}/*`]
        })
      ]
    })
  }
});
```

#### Knowledge Base Configuration
```python
# buildspec.yml - Knowledge Base creation with Neptune integration
KB_ID=$(aws bedrock-agent create-knowledge-base \
  --name "${PROJECT_NAME}-knowledge-base" \
  --role-arn "$KB_ROLE_ARN" \
  --knowledge-base-configuration '{
    "type": "VECTOR",
    "vectorKnowledgeBaseConfiguration": {
      "embeddingModelArn": "arn:aws:bedrock:'"$CDK_DEFAULT_REGION"'::foundation-model/amazon.titan-embed-text-v2:0"
    }
  }' \
  --storage-configuration '{
    "type": "NEPTUNE_ANALYTICS",
    "neptuneAnalyticsConfiguration": {
      "graphArn": "'"$GRAPH_ARN"'",
      "fieldMapping": {
        "metadataField": "metadata",
        "textField": "text"
      }
    }
  }' \
  --region "$CDK_DEFAULT_REGION" \
  --query 'knowledgeBase.knowledgeBaseId' --output text)
```

### 5. Data Source Integration with Custom Transformation

#### S3 Data Source Configuration
```python
# Data Source with custom transformation lambda
DS_ID=$(aws bedrock-agent create-data-source \
  --name "${PROJECT_NAME}-s3-datasource" \
  --knowledge-base-id "$KB_ID" \
  --data-source-configuration '{
    "type": "S3",
    "s3Configuration": {
      "bucketArn": "'"$BUCKET_ARN"'",
      "inclusionPrefixes": ["extracted/"]
    }
  }' \
  --vector-ingestion-configuration '{
    "chunkingConfiguration": {
      "chunkingStrategy": "FIXED_SIZE",
      "fixedSizeChunkingConfiguration": {
        "maxTokens": 1500,
        "overlapPercentage": 20
      }
    },
    "customTransformationConfiguration": {
      "intermediateStorage": {
        "s3Location": {"uri": "s3://'"$TRANSFORMATION_BUCKET"'/temp/"}
      },
      "transformations": [{
        "stepToApply": "POST_CHUNKING",
        "transformationFunction": {
          "transformationLambdaConfiguration": {
            "lambdaArn": "'"$TRANSFORMATION_LAMBDA_ARN"'"
          }
        }
      }]
    }
  }')
```

#### Custom Transformation Lambda
```python
# lambda/kb-transformation/lambda_function.py

def lambda_handler(event, context):
    """Custom transformation for Knowledge Base ingestion"""
    
    # 1. Parse the transformation request
    request_body = json.loads(event['body'])
    input_files = request_body['inputFiles']
    
    transformed_files = []
    
    for file_info in input_files:
        # 2. Download file from S3
        s3_key = file_info['uri'].split('/')[-1]
        file_content = download_from_s3(file_info['uri'])
        
        # 3. Apply custom transformations
        # - Filter out irrelevant content
        # - Enhance metadata
        # - Normalize text format
        transformed_content = apply_transformations(file_content, file_info['metadata'])
        
        # 4. Upload transformed content
        output_uri = upload_transformed_content(transformed_content, s3_key)
        
        transformed_files.append({
            'uri': output_uri,
            'metadata': enhanced_metadata
        })
    
    return {
        'statusCode': 200,
        'body': json.dumps({
            'outputFiles': transformed_files
        })
    }

def apply_transformations(content, metadata):
    """Apply document-specific transformations"""
    
    # Filter out boilerplate text
    if 'congress' in metadata:
        content = remove_congressional_boilerplate(content)
    
    # Enhance with structured metadata
    enhanced_content = f"""
    Document Type: {metadata.get('document_type', 'Unknown')}
    Congress: {metadata.get('congress', 'Unknown')}
    Bill Type: {metadata.get('bill_type', 'Unknown')}
    Bill Number: {metadata.get('bill_number', 'Unknown')}
    
    Content:
    {content}
    """
    
    return enhanced_content
```

### 6. Event-Driven Integration Flow

#### S3 Event Triggers
```typescript
// CDK - S3 event integration
dataBucket.addEventNotification(
  s3.EventType.OBJECT_CREATED,
  new s3n.LambdaDestination(kbSyncTriggerFunction),
  { prefix: 'extracted/' }
);
```

#### Knowledge Base Sync Integration
```python
# lambda/kb-sync-trigger/lambda_function.py

def lambda_handler(event, context):
    """Triggered by S3 events to sync Knowledge Base"""
    
    # 1. Parse S3 event
    for record in event['Records']:
        bucket = record['s3']['bucket']['name']
        key = record['s3']['object']['key']
        
        # 2. Check if this is a significant update (batch completion)
        if should_trigger_sync(key):
            # 3. Start Knowledge Base ingestion job
            response = bedrock_agent.start_ingestion_job(
                knowledgeBaseId=os.environ['KNOWLEDGE_BASE_ID'],
                dataSourceId=os.environ['DATA_SOURCE_ID']
            )
            
            job_id = response['ingestionJob']['ingestionJobId']
            
            # 4. Monitor job progress (optional)
            monitor_ingestion_job(job_id)
    
    return {'statusCode': 200}

def should_trigger_sync(s3_key):
    """Determine if we should trigger KB sync based on file pattern"""
    # Only sync when we have a complete batch
    # e.g., when congress collection is complete
    return 'congress_' in s3_key and s3_key.endswith('_complete.marker')
```

### 7. Frontend Integration

#### API Integration Pattern
```javascript
// frontend/index.html - API integration

class ChatInterface {
    constructor() {
        this.apiEndpoint = localStorage.getItem('apiEndpoint') || '';
        this.selectedPersona = localStorage.getItem('persona') || 'general';
    }
    
    async sendMessage(question) {
        try {
            // 1. Prepare request with persona
            const requestBody = {
                question: question,
                persona: this.selectedPersona
            };
            
            // 2. Call API Gateway endpoint
            const response = await fetch(`${this.apiEndpoint}/chat`, {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                },
                body: JSON.stringify(requestBody)
            });
            
            // 3. Handle response
            const data = await response.json();
            
            if (response.ok) {
                this.displayResponse(data);
            } else {
                this.displayError(data.error || 'An error occurred');
            }
            
        } catch (error) {
            this.displayError('Network error: ' + error.message);
        }
    }
    
    displayResponse(data) {
        // Display persona-tailored response with citations
        const responseHtml = `
            <div class="response">
                <div class="answer">${data.answer}</div>
                <div class="citations">
                    ${data.citations.map(citation => 
                        `<div class="citation">${citation.title} - ${citation.source}</div>`
                    ).join('')}
                </div>
            </div>
        `;
        
        document.getElementById('chatMessages').innerHTML += responseHtml;
    }
}
```

### 8. Monitoring Integration

#### CloudWatch Integration Pattern
```python
# Structured logging across all components

import logging
import json

# Configure structured logging
logger = logging.getLogger()
logger.setLevel(logging.INFO)

def log_structured(level, message, **kwargs):
    """Structured logging for CloudWatch"""
    log_entry = {
        'timestamp': datetime.utcnow().isoformat(),
        'level': level,
        'message': message,
        'component': os.environ.get('COMPONENT_NAME', 'unknown'),
        **kwargs
    }
    
    if level == 'ERROR':
        logger.error(json.dumps(log_entry))
    elif level == 'WARNING':
        logger.warning(json.dumps(log_entry))
    else:
        logger.info(json.dumps(log_entry))

# Usage throughout the system
log_structured('INFO', 'Processing document', 
               document_id=doc_id, 
               file_size=file_size,
               processing_method='textract_sync')

log_structured('ERROR', 'Textract processing failed',
               document_id=doc_id,
               error_code=error.response['Error']['Code'],
               retry_count=retry_count)
```

## System Components

This integration architecture ensures that all components work together seamlessly, with proper error handling, monitoring, and scalability built into each integration point.

### 1. Data Collection Pipeline

#### Fargate Collector (`fargate/collect_bills.py`)
Containerized application that collects documents from multiple sources:

**Data Sources:**
- **Congress API**: Legislative bills from Congress 1-16 (all 8 bill types)
- **Chronicling America API**: Historical newspapers (1760-1820)

**Processing Flow:**
```python
# Data collection workflow
PDF URL → Download → Textract Processing → Metadata Extraction → S3 Storage
```

**Key Features:**
- Dual-source collection capability
- Smart file size handling (sync/async Textract APIs)
- Rate limiting compliance (1 TPS sync, 2 TPS async)
- Automatic retry with exponential backoff
- Comprehensive error handling and logging

#### Text Extraction Strategy
```python
# File size-based API selection
if file_size <= 5MB:
    use_textract_sync_api()  # Faster, 1 page limit
else:
    use_textract_async_api()  # Slower, 3000 page limit
```

### 2. Lambda Functions

#### Chat Handler (`lambda/chat-handler/lambda_function.py`)
Processes user queries and generates persona-based responses.

**Features:**
- **4 Persona Types**: Congressional Staffer, Research Journalist, Law Student, General
- **Bill-specific filtering**: Extracts congress/bill info from natural language
- **Metadata-based search**: Precise document retrieval
- **Citation management**: Source tracking and attribution

**Query Processing:**
```python
def lambda_handler(event, context):
    # 1. Extract persona and question
    # 2. Parse bill information (if any)
    # 3. Build metadata filters
    # 4. Query Knowledge Base
    # 5. Generate persona-specific response
```

#### Fargate Trigger (`lambda/fargate-trigger/`)
Manages ECS task execution for data collection.

**Responsibilities:**
- Launch Fargate tasks with proper configuration
- Environment variable management
- Task status monitoring
- Error reporting and retry logic

#### KB Sync Trigger (`lambda/kb-sync-trigger/`)
Manages Knowledge Base synchronization.

**Functions:**
- Trigger ingestion jobs after data collection
- Monitor sync progress
- Handle ingestion failures
- Update Lambda environment variables

#### KB Transformation (`lambda/kb-transformation/`)
Custom document preprocessing pipeline.

**Processing Steps:**
- Content filtering for relevance
- Metadata enrichment
- Document chunking optimization
- Entity extraction preparation

### 3. Knowledge Base Architecture

#### Neptune Analytics Configuration
```typescript
// Vector search configuration
vectorSearchConfiguration: {
  dimension: 1024,  // Titan Embed v2 dimensions
  similarity: "cosine"
}

// Graph configuration
provisionedMemory: 16,  // m-NCUs
publicConnectivity: false
```

#### Document Processing Pipeline
```typescript
// Custom transformation pipeline
vectorIngestionConfiguration: {
  chunkingConfiguration: {
    chunkingStrategy: "FIXED_SIZE",
    fixedSizeChunkingConfiguration: {
      maxTokens: 1500,
      overlapPercentage: 20
    }
  },
  customTransformationConfiguration: {
    transformations: [{
      stepToApply: "POST_CHUNKING",
      transformationFunction: {
        transformationLambdaConfiguration: {
          lambdaArn: transformationLambdaArn
        }
      }
    }]
  }
}
```

## Advanced Features

### 1. Persona-Based Responses

The system provides tailored responses based on user roles:

```python
PERSONAS = {
    'congressional_staffer': {
        'style': 'Formal and authoritative',
        'focus': 'Policy implications and precedents',
        'citations': 'Detailed with specific references'
    },
    'research_journalist': {
        'style': 'Engaging narrative',
        'focus': 'Historical context and human interest',
        'citations': 'Accessible with background information'
    },
    'law_student': {
        'style': 'Educational and comprehensive',
        'focus': 'Legal reasoning and case analysis',
        'citations': 'Academic with case law references'
    },
    'general': {
        'style': 'Clear and informative',
        'focus': 'Balanced overview',
        'citations': 'Simplified but accurate'
    }
}
```

### 2. Intelligent Document Filtering

Natural language processing extracts structured queries:

```python
def extract_bill_info(question: str) -> dict:
    # Patterns for bill identification
    patterns = [
        r'(?:bill\s+)?([hs]\.?(?:res|con\.res|j\.res)?\.?\s*r?\.?\s*)(\d+)',
        r'(?:congress\s+)?(\d+).*?(?:bill\s+)?([hs]\.?(?:res|con\.res|j\.res)?\.?\s*r?\.?\s*)(\d+)',
        # ... more patterns
    ]
    # Returns: {'congress': '6', 'bill_type': 'hr', 'bill_number': '1'}
```

### 3. Metadata-Based Search

Precise document retrieval using structured filters:

```python
def build_metadata_filter(bill_info: dict) -> dict:
    filters = []
    
    if 'congress' in bill_info:
        filters.append({
            "equals": {"key": "congress", "value": bill_info['congress']}
        })
    
    if 'bill_type' in bill_info:
        filters.append({
            "equals": {"key": "bill_type", "value": bill_info['bill_type']}
        })
    
    return {"andAll": filters} if len(filters) > 1 else filters[0]
```

## Data Flow

### Query Processing Flow
```
1. User Query → API Gateway → Chat Handler Lambda
2. Extract persona and bill information
3. Build metadata filters for precise search
4. Query Knowledge Base with filters
5. Retrieve relevant document chunks
6. Generate persona-specific response using Claude
7. Return formatted response with citations
```

### Data Collection Flow
```
1. Manual/Scheduled trigger → Fargate Trigger Lambda
2. Launch ECS Fargate task with environment config
3. Fargate downloads PDFs from Congress/Chronicling America APIs
4. Process documents through Textract (sync/async based on size)
5. Extract text and preserve metadata
6. Upload processed documents to S3 with organized structure
7. Trigger Knowledge Base sync automatically
8. Knowledge Base ingests documents and creates embeddings
9. System ready for queries
```

## Storage Organization

### S3 Bucket Structure
```
s3://bucket-name/
├── extracted/
│   ├── congress_1/
│   │   ├── hr/
│   │   ├── s/
│   │   └── ...
│   ├── congress_2/
│   └── newspapers_1815/
├── temp/
│   └── textract-processing/
└── builds/
    └── frontend-artifacts/
```

### Metadata Schema
```json
{
  "congress": "6",
  "bill_type": "hr",
  "bill_number": "1",
  "title": "Bill Title",
  "date": "1799-12-02",
  "source": "congress_api",
  "document_type": "bill",
  "text_length": 1234,
  "extraction_method": "textract_sync"
}
```

## Performance & Scalability

### Cost Optimization
- **Textract**: ~$1.50 per 1,000 pages (vs $40-200 for BDA)
- **Serverless**: Pay-per-use Lambda functions
- **Neptune Analytics**: Right-sized at 16 m-NCUs
- **S3**: Intelligent tiering for long-term storage

### Rate Limiting & Throttling
```python
# API rate limits compliance
TEXTRACT_SYNC_DELAY = 1.0    # 1 TPS
TEXTRACT_ASYNC_DELAY = 0.5   # 2 TPS
CONGRESS_API_DELAY = 0.1     # 10 TPS
```

### Error Handling Strategy
- **Graceful degradation**: Continue processing on individual failures
- **Exponential backoff**: Automatic retry with increasing delays
- **Circuit breaker**: Stop processing on repeated failures
- **Comprehensive logging**: CloudWatch integration for debugging

## Deployment Integration & Automation

### Automated Deployment Pipeline

The `buildspec.yml` orchestrates a complex deployment process that integrates multiple AWS services:

```yaml
# buildspec.yml - Automated deployment integration

phases:
  install:
    # 1. Environment setup
    runtime-versions:
      nodejs: 20
      python: 3.11
    commands:
      - npm install -g aws-cdk@latest  # CDK CLI for infrastructure
      - cd backend && npm install      # TypeScript dependencies

  pre_build:
    # 2. Infrastructure preparation
    commands:
      - npm run build                  # Compile TypeScript
      - cdk bootstrap --require-approval never  # CDK environment setup
      - cd fargate && chmod +x build.sh && ./build.sh  # Docker image build

  build:
    # 3. Integrated deployment process
    commands:
      # Deploy infrastructure first
      - cdk deploy LOCstack --require-approval never --outputs-file outputs.json
      
      # Extract deployment outputs for integration
      - DATA_BUCKET=$(cat outputs.json | jq -r '.LOCstack.DataBucketName')
      - API_URL=$(cat outputs.json | jq -r '.LOCstack.APIGatewayURL')
      - KB_ROLE_ARN=$(cat outputs.json | jq -r '.LOCstack.KnowledgeBaseRoleArn')
      
      # Frontend build and integration
      - cd ../frontend
      - echo "REACT_APP_API_BASE_URL=$API_URL" > .env.production
      - npm install && npm run build
      - aws s3 cp build.zip "s3://$BUILDS_BUCKET/builds/build-$(date +%s).zip"
      
      # Knowledge Base integration
      - aws bedrock-agent create-knowledge-base --name "${PROJECT_NAME}-knowledge-base"
      - aws bedrock-agent create-data-source --knowledge-base-id "$KB_ID"
      
      # Lambda environment variable updates
      - aws lambda update-function-configuration --function-name "${PROJECT_NAME}-chat-handler"
      
      # Trigger initial data collection
      - aws lambda invoke --function-name "${PROJECT_NAME}-fargate-trigger"
```

### Resource Dependency Management

The CDK stack manages complex resource dependencies:

```typescript
// chronicling-america-stack.ts - Dependency management

export class ChroniclingAmericaStack extends cdk.Stack {
  constructor(scope: Construct, id: string, props?: cdk.StackProps) {
    super(scope, id, props);

    // 1. Foundation resources (VPC, Security Groups)
    const vpc = new ec2.Vpc(this, "VPC", { maxAzs: 2 });
    const securityGroup = new ec2.SecurityGroup(this, "SecurityGroup", { vpc });

    // 2. Storage resources (S3 buckets)
    const dataBucket = new s3.Bucket(this, "DataBucket");
    const transformationBucket = new s3.Bucket(this, "TransformationBucket");

    // 3. Compute resources (ECS, Lambda) - depend on VPC
    const cluster = new ecs.Cluster(this, "Cluster", { 
      vpc,
      containerInsights: true 
    });

    // 4. Lambda functions with proper IAM roles
    const chatHandler = new PythonFunction(this, "ChatHandlerFunction", {
      entry: path.join(__dirname, "../lambda/chat-handler"),
      vpc,  // VPC integration for security
      environment: {
        // Environment variables updated post-deployment
        KNOWLEDGE_BASE_ID: "",  // Populated by buildspec.yml
        MODEL_ID: bedrockModelId
      }
    });

    // 5. API Gateway integration
    const api = new apigateway.RestApi(this, "Api");
    const chatResource = api.root.addResource("chat");
    chatResource.addMethod("POST", new apigateway.LambdaIntegration(chatHandler));

    // 6. ECS Task Definition with proper resource allocation
    const taskDefinition = new ecs.FargateTaskDefinition(this, "TaskDefinition", {
      memoryLimitMiB: 4096,
      cpu: 2048
    });

    taskDefinition.addContainer("collector", {
      image: ecs.ContainerImage.fromAsset(path.join(__dirname, "../fargate")),
      environment: {
        BUCKET_NAME: dataBucket.bucketName,
        REGION: this.region
      },
      logging: ecs.LogDrivers.awsLogs({
        streamPrefix: "collector",
        logGroup: new logs.LogGroup(this, "CollectorLogGroup", {
          retention: logs.RetentionDays.ONE_WEEK
        })
      })
    });

    // 7. IAM role integration for cross-service access
    const knowledgeBaseRole = new iam.Role(this, "KnowledgeBaseRole", {
      assumedBy: new iam.ServicePrincipal("bedrock.amazonaws.com"),
      inlinePolicies: {
        S3Access: new iam.PolicyDocument({
          statements: [
            new iam.PolicyStatement({
              effect: iam.Effect.ALLOW,
              actions: ["s3:GetObject", "s3:ListBucket"],
              resources: [dataBucket.bucketArn, `${dataBucket.bucketArn}/*`]
            })
          ]
        })
      }
    });

    // 8. Output values for post-deployment integration
    new cdk.CfnOutput(this, "DataBucketName", { value: dataBucket.bucketName });
    new cdk.CfnOutput(this, "APIGatewayURL", { value: api.url });
    new cdk.CfnOutput(this, "KnowledgeBaseRoleArn", { value: knowledgeBaseRole.roleArn });
  }
}
```

### Cross-Service Communication Patterns

#### 1. Lambda to ECS Integration
```python
# lambda/fargate-trigger/lambda_function.py

def trigger_fargate_task():
    """Launch ECS Fargate task with proper networking and environment"""
    
    ecs_client = boto3.client('ecs')
    
    # Task launch with VPC integration
    response = ecs_client.run_task(
        cluster=os.environ['CLUSTER_NAME'],
        taskDefinition=os.environ['TASK_DEFINITION_ARN'],
        launchType='FARGATE',
        networkConfiguration={
            'awsvpcConfiguration': {
                'subnets': [os.environ['SUBNET_ID']],
                'securityGroups': [os.environ['SECURITY_GROUP_ID']],
                'assignPublicIp': 'ENABLED'  # For internet access
            }
        },
        overrides={
            'containerOverrides': [{
                'name': 'collector',
                'environment': [
                    {'name': 'START_CONGRESS', 'value': '1'},
                    {'name': 'END_CONGRESS', 'value': '16'},
                    {'name': 'BUCKET_NAME', 'value': os.environ['BUCKET_NAME']},
                    {'name': 'KNOWLEDGE_BASE_ID', 'value': os.environ['KNOWLEDGE_BASE_ID']}
                ]
            }]
        }
    )
    
    return response['tasks'][0]['taskArn']
```

#### 2. S3 to Knowledge Base Integration
```python
# Automatic Knowledge Base sync on S3 events

def handle_s3_event(event, context):
    """Process S3 events and trigger Knowledge Base sync"""
    
    for record in event['Records']:
        # Parse S3 event
        bucket = record['s3']['bucket']['name']
        key = record['s3']['object']['key']
        
        # Only process files in extracted/ prefix
        if key.startswith('extracted/'):
            # Check if this completes a batch
            if is_batch_complete(bucket, key):
                # Trigger Knowledge Base ingestion
                bedrock_agent = boto3.client('bedrock-agent')
                
                response = bedrock_agent.start_ingestion_job(
                    knowledgeBaseId=os.environ['KNOWLEDGE_BASE_ID'],
                    dataSourceId=os.environ['DATA_SOURCE_ID']
                )
                
                # Monitor job progress
                job_id = response['ingestionJob']['ingestionJobId']
                monitor_ingestion_job(job_id)
```

#### 3. API Gateway to Lambda Integration
```python
# Standardized API response format across all Lambda functions

def create_api_response(status_code, body, headers=None):
    """Standardized API Gateway response format"""
    
    default_headers = {
        'Content-Type': 'application/json',
        'Access-Control-Allow-Origin': '*',
        'Access-Control-Allow-Methods': 'POST, GET, OPTIONS',
        'Access-Control-Allow-Headers': 'Content-Type'
    }
    
    if headers:
        default_headers.update(headers)
    
    return {
        'statusCode': status_code,
        'headers': default_headers,
        'body': json.dumps(body) if isinstance(body, dict) else body
    }

# Usage in Lambda functions
def lambda_handler(event, context):
    try:
        # Process request
        result = process_request(event)
        return create_api_response(200, result)
        
    except Exception as e:
        logger.error(f"Error processing request: {str(e)}")
        return create_api_response(500, {'error': 'Internal server error'})
```

### Environment Variable Management

#### Dynamic Configuration Updates
```bash
# buildspec.yml - Dynamic Lambda configuration

# Update chat-handler with Knowledge Base ID
CHAT_ENV=$(aws lambda get-function-configuration \
  --function-name "${PROJECT_NAME}-chat-handler" \
  --query 'Environment.Variables' --output json)

echo "$CHAT_ENV" | jq \
  --arg kb_id "$KB_ID" \
  --arg model_id "$BEDROCK_MODEL_ID" \
  '{Variables: (. + {KNOWLEDGE_BASE_ID: $kb_id, MODEL_ID: $model_id})}' > /tmp/chat_env.json

aws lambda update-function-configuration \
  --function-name "${PROJECT_NAME}-chat-handler" \
  --environment file:///tmp/chat_env.json
```

#### Configuration Validation
```python
# Environment variable validation across components

import os
import sys

def validate_environment():
    """Validate required environment variables"""
    
    required_vars = {
        'BUCKET_NAME': 'S3 bucket for document storage',
        'KNOWLEDGE_BASE_ID': 'Bedrock Knowledge Base identifier',
        'MODEL_ID': 'Bedrock model identifier',
        'REGION': 'AWS region'
    }
    
    missing_vars = []
    for var, description in required_vars.items():
        if not os.environ.get(var):
            missing_vars.append(f"{var} ({description})")
    
    if missing_vars:
        logger.error(f"Missing required environment variables: {missing_vars}")
        sys.exit(1)
    
    logger.info("Environment validation successful")

# Call validation in all Lambda functions and Fargate tasks
validate_environment()
```

## Deployment Strategy

### CI/CD Pipeline (`buildspec.yml`)

The deployment process is fully automated:

```yaml
phases:
  install:
    - Install CDK CLI and dependencies
    - Build TypeScript infrastructure code
  
  pre_build:
    - Bootstrap CDK environment
    - Build and push Fargate Docker image
  
  build:
    - Deploy CDK stack (infrastructure)
    - Build and deploy React frontend
    - Create Neptune Analytics graph
    - Set up Bedrock Knowledge Base
    - Configure data sources and transformations
    - Update Lambda environment variables
    - Trigger initial data collection
  
  post_build:
    - Validate deployment
    - Output access URLs and monitoring commands
```

### Idempotent Deployment

The system supports safe redeployment:

- **Resource reuse**: Checks for existing Neptune graphs and Knowledge Bases
- **Configuration updates**: Updates Lambda environment variables
- **Frontend automation**: Builds and deploys React app to Amplify
- **Data preservation**: Maintains existing S3 data and metadata

## Monitoring & Observability

### CloudWatch Integration
- **Lambda metrics**: Execution duration, error rates, memory usage
- **ECS metrics**: Task execution status, resource utilization
- **API Gateway**: Request/response logs, latency metrics
- **Textract**: API call volume, processing times, error rates

### Logging Strategy
```python
# Structured logging throughout the system
logger.info(f"Processing document: {document_id}")
logger.warning(f"Textract rate limit hit, waiting {delay}s")
logger.error(f"Failed to process {document_id}: {error}")
```

### Key Metrics to Monitor
- Document processing rate (docs/hour)
- Text extraction success rate
- Knowledge Base sync duration
- Query response times
- Cost per document processed

## Security Considerations

### IAM Roles & Policies
- **Principle of least privilege**: Each component has minimal required permissions
- **Cross-service access**: Secure communication between Lambda, ECS, and Bedrock
- **API security**: API Gateway with proper authentication

### Data Protection
- **Encryption at rest**: S3 and Neptune Analytics encryption
- **Encryption in transit**: HTTPS/TLS for all communications
- **Access logging**: CloudTrail integration for audit trails

## Future Enhancements

### Planned Improvements
1. **Multi-modal support**: Image and table extraction from documents
2. **Real-time ingestion**: Stream processing for new documents
3. **Advanced analytics**: Document relationship mapping
4. **Performance optimization**: Caching layer for frequent queries
5. **Multi-region deployment**: Global availability and disaster recovery

### Scalability Roadmap
- **Auto-scaling**: Dynamic ECS task scaling based on queue depth
- **Caching**: ElastiCache for frequent query results
- **CDN**: CloudFront for global frontend distribution
- **Database optimization**: Neptune Analytics cluster scaling

This architecture provides a robust, scalable foundation for historical document analysis with modern AI capabilities, designed to serve both researchers and general users effectively.